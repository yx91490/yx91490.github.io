(window.webpackJsonp=window.webpackJsonp||[]).push([[55],{442:function(e,a,t){"use strict";t.r(a);var s=t(10),r=Object(s.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"hdfs命令笔记"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hdfs命令笔记"}},[e._v("$")]),e._v(" HDFS命令笔记")]),e._v(" "),a("h3",{attrs:{id:"hadoop-help"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-help"}},[e._v("$")]),e._v(" hadoop --help")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop [--config confdir] COMMAND\n       where COMMAND is one of:\n  fs                   run a generic filesystem user client\n  version              print the version\n  jar <jar>            run a jar file\n  checknative [-a|-h]  check native hadoop and compression libraries availability\n  distcp <srcurl> <desturl> copy file or directories recursively\n  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\n  classpath            prints the class path needed to get the\n  credential           interact with credential providers\n                       Hadoop jar and the required libraries\n  daemonlog            get/set the log level for each daemon\n  s3guard              manage data on S3\n  trace                view and modify Hadoop tracing settings\n or\n  CLASSNAME            run the class named CLASSNAME\n\nMost commands print help when invoked w/o parameters.\n")])])]),a("blockquote",[a("p",[e._v("注：当调用不带参数的命令时，大多数命令都会显示帮助。")])]),e._v(" "),a("h3",{attrs:{id:"hadoop-version"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hadoop-version"}},[e._v("$")]),e._v(" hadoop version")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hadoop version\nHadoop 2.6.0-cdh5.16.2\nSubversion http://github.com/cloudera/hadoop -r 4f94d60caa4cbb9af0709a2fd96dc3861af9cf20\nCompiled by jenkins on 2019-06-03T10:41Z\nCompiled with protoc 2.5.0\nFrom source with checksum 79b9b24a29c6358b53597c3b49575e37\nThis command was run using /opt/cloudera/parcels/CDH-5.16.2-1.cdh5.16.2.p0.8/jars/hadoop-common-2.6.0-cdh5.16.2.jar\n")])])]),a("h3",{attrs:{id:"hdfs-fsck"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hdfs-fsck"}},[e._v("$")]),e._v(" hdfs fsck")]),e._v(" "),a("p",[e._v("检查Hadoop文件系统的健康状况：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hdfs fsck /\nConnecting to namenode via http://hdp0.local.org:50070/fsck?ugi=hdfs&path=%2F\nFSCK started by hdfs (auth:SIMPLE) from /192.168.12.213 for path / at Thu Apr 09 12:52:43 CST 2020\n........................................................................................................................................................................................Status: HEALTHY\n Total size:\t222969126092 B (Total open files size: 249 B)\n Total dirs:\t14477\n Total files:\t32794\n Total symlinks:\t\t0 (Files currently being written: 4)\n Total blocks (validated):\t25453 (avg. block size 8760033 B) (Total open file blocks (not validated): 3)\n Minimally replicated blocks:\t25453 (100.0 %)\n Over-replicated blocks:\t0 (0.0 %)\n Under-replicated blocks:\t0 (0.0 %)\n Mis-replicated blocks:\t\t0 (0.0 %)\n Default replication factor:\t3\n Average block replication:\t2.9756021\n Corrupt blocks:\t\t0\n Missing replicas:\t\t0 (0.0 %)\n Number of data-nodes:\t\t3\n Number of racks:\t\t1\nFSCK ended at Thu Apr 09 12:52:44 CST 2020 in 1306 milliseconds\n\n\nThe filesystem under path '/' is HEALTHY\n")])])]),a("p",[e._v("根据blockId查找所属文件：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hdfs fsck -blockId <block_id>\n\nhdfs fsck -blockId blk_1100790203\nConnecting to namenode \nFSCK started by hdfs \n\nBlock Id: blk_1100790203\nBlock belongs to: /common/FFL1447685899336.txt\n")])])]),a("p",[e._v("参考："),a("a",{attrs:{href:"https://stackoverflow.com/questions/10881449/how-to-find-file-from-blockname-in-hdfs-hadoop",target:"_blank",rel:"noopener noreferrer"}},[e._v("how to find file from blockName in HDFS hadoop?"),a("OutboundLink")],1)]),e._v(" "),a("h3",{attrs:{id:"hdfs-balancer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#hdfs-balancer"}},[e._v("$")]),e._v(" hdfs balancer")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hdfs balancer\n\t[-policy <policy>]\tthe balancing policy: datanode or blockpool\n\t[-threshold <threshold>]\tPercentage of disk capacity\n\t[-exclude [-f <hosts-file> | <comma-separated list of hosts>]]\tExcludes the specified datanodes.\n\t[-include [-f <hosts-file> | <comma-separated list of hosts>]]\tIncludes only the specified datanodes.\n\t[-source [-f <hosts-file> | <comma-separated list of hosts>]]\tPick only the specified datanodes as source nodes.\n\t[-idleiterations <idleiterations>]\tNumber of consecutive idle iterations (-1 for Infinite) before exit.\n\t[-runDuringUpgrade]\tWhether to run the balancer during an ongoing HDFS upgrade.This is usually not desired since it will not affect used space on over-utilized machines.\n\nGeneric options supported are\n-conf <configuration file>     specify an application configuration file\n-D <property=value>            use value for given property\n-fs <local|namenode:port>      specify a namenode\n-jt <local|resourcemanager:port>    specify a ResourceManager\n-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n\nThe general command line syntax is\nbin/hadoop command [genericOptions] [commandOptions]\n")])])]),a("p",[e._v("Output:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hdfs balancer\n20/04/14 20:03:12 INFO balancer.Balancer: namenodes  = [hdfs://nameservice1]\n20/04/14 20:03:12 INFO balancer.Balancer: parameters = Balancer.Parameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, run during upgrade = false]\n20/04/14 20:03:12 INFO balancer.Balancer: included nodes = []\n20/04/14 20:03:12 INFO balancer.Balancer: excluded nodes = []\n20/04/14 20:03:12 INFO balancer.Balancer: source nodes = []\nTime Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved\n20/04/14 20:03:13 INFO balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)\n20/04/14 20:03:13 INFO balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)\n20/04/14 20:03:13 INFO balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)\n20/04/14 20:03:13 INFO balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)\n20/04/14 20:03:13 INFO balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)\n20/04/14 20:03:13 INFO net.NetworkTopology: Adding a new node: /default/192.168.12.212:50010\n20/04/14 20:03:13 INFO net.NetworkTopology: Adding a new node: /default/192.168.12.211:50010\n20/04/14 20:03:13 INFO net.NetworkTopology: Adding a new node: /default/192.168.12.213:50010\n20/04/14 20:03:13 INFO balancer.Balancer: 0 over-utilized: []\n20/04/14 20:03:13 INFO balancer.Balancer: 0 underutilized: []\nThe cluster is balanced. Exiting...\n2020-4-14 20:03:13                0                  0 B                 0 B               -1 B\n2020-4-14 20:03:13       Balancing took 1.833 seconds\n")])])]),a("h2",{attrs:{id:"fs命令"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fs命令"}},[e._v("$")]),e._v(" fs命令")]),e._v(" "),a("h3",{attrs:{id:"ls"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ls"}},[e._v("$")]),e._v(" ls")]),e._v(" "),a("p",[e._v("使用帮助：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]\n")])])]),a("p",[e._v("按修改时间倒序排序：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("hadoop fs -ls -r -t /path\n")])])]),a("p",[e._v("只列出文件名称：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("hadoop fs -ls -C /path\n")])])]),a("p",[a("code",[e._v("hadoop fs -ls")]),e._v("输出：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hdfs dfs -ls /\nFound 5 items\ndrwxr-xr-x   - flink hdfs                0 2019-09-04 14:51 /flink\ndrwxr-xr-x   - hbase hbase               0 2020-02-26 12:55 /hbase\ndrwxr-xr-x   - hdfs  supergroup          0 2019-09-20 13:56 /system\ndrwxrwxrwt   - hdfs  supergroup          0 2020-04-09 11:04 /tmp\ndrwxr-xr-x   - hdfs  supergroup          0 2019-09-11 21:02 /user\n")])])]),a("h3",{attrs:{id:"count"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#count"}},[e._v("$")]),e._v(" count")]),e._v(" "),a("p",[e._v("统计符合特定模式的路径下的目录数、文件数和字节数：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hdfs dfs -count /\n       14477        32802       222969126341 /\n")])])]),a("h3",{attrs:{id:"mkdir"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mkdir"}},[e._v("$")]),e._v(" mkdir")]),e._v(" "),a("p",[e._v("创建HDFS目录：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -mkdir [-p] <path> ...\n")])])]),a("h3",{attrs:{id:"mv"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#mv"}},[e._v("$")]),e._v(" mv")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -mv <src> ... <dst>\n")])])]),a("h3",{attrs:{id:"cp"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cp"}},[e._v("$")]),e._v(" cp")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -cp <src> ... <dst>\n")])])]),a("h3",{attrs:{id:"copyfromlocal"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#copyfromlocal"}},[e._v("$")]),e._v(" copyFromLocal")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>\n")])])]),a("h3",{attrs:{id:"copytolocal"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#copytolocal"}},[e._v("$")]),e._v(" copyToLocal")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>\n")])])]),a("h3",{attrs:{id:"put"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#put"}},[e._v("$")]),e._v(" put")]),e._v(" "),a("p",[e._v("把一个或者多个本地文件系统路径（文件或目录）上传到HDFS：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -put [-f] [-p] [-l] <localsrc> ... <dst>\n")])])]),a("h3",{attrs:{id:"rm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#rm"}},[e._v("$")]),e._v(" rm")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -rm [-f] [-r|-R] [-skipTrash] <src> ...\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hdfs dfs -rm /hadoop/test\n\n16/11/07 01:53:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n\nDeleted /hadoop/test\n")])])]),a("p",[e._v("跳过回收站删除文件：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("hadoop rm -r -skipTrash /path\n")])])]),a("h3",{attrs:{id:"getmerge"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#getmerge"}},[e._v("$")]),e._v(" getmerge")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -getmerge [-nl] <src> <localdst>\n")])])]),a("h2",{attrs:{id:"get"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#get"}},[e._v("$")]),e._v(" get")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>\n")])])]),a("h3",{attrs:{id:"cat"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cat"}},[e._v("$")]),e._v(" cat")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -cat [-ignoreCrc] <src> ...\n")])])]),a("h3",{attrs:{id:"tail"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tail"}},[e._v("$")]),e._v(" tail")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -tail [-f] <file>\n")])])]),a("h3",{attrs:{id:"text"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#text"}},[e._v("$")]),e._v(" text")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -text [-ignoreCrc] <src> ...\n")])])]),a("h3",{attrs:{id:"touchz"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#touchz"}},[e._v("$")]),e._v(" touchz")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -touchz <path> ...\n")])])]),a("h3",{attrs:{id:"chmod"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#chmod"}},[e._v("$")]),e._v(" chmod")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...\n")])])]),a("h3",{attrs:{id:"chown"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#chown"}},[e._v("$")]),e._v(" chown")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -chown [-R] [OWNER][:[GROUP]] PATH...\n")])])]),a("h3",{attrs:{id:"df"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#df"}},[e._v("$")]),e._v(" df")]),e._v(" "),a("p",[e._v("展示给定hdfs目标的剩余空间大小：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hdfs dfs -df hdfs:/\nFilesystem                    Size          Used     Available  Use%\nhdfs://nameservice1  1241336426496  674560343997  389376255958   54%\n")])])]),a("h3",{attrs:{id:"du"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#du"}},[e._v("$")]),e._v(" du")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -du [-s] [-h] [-x] <path> ...\n")])])]),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hadoop fs -du /user/flink\n0            0             /user/flink/.Trash\n34500398160  103501194480  /user/flink/.flink\n")])])]),a("p",[e._v("按大小逆序输出子目录及文件的大小：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("hadoop fs -du -s /path/* |sort -rnk1\n")])])]),a("h3",{attrs:{id:"stat"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#stat"}},[e._v("$")]),e._v(" stat")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -stat [format] <path> ...\n")])])]),a("h3",{attrs:{id:"getfacl"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#getfacl"}},[e._v("$")]),e._v(" getfacl")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -getfacl [-R] <path>\n")])])]),a("h3",{attrs:{id:"getfattr"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#getfattr"}},[e._v("$")]),e._v(" getfattr")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -getfattr [-R] {-n name | -d} [-e en] <path>\n")])])]),a("h3",{attrs:{id:"expunge"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#expunge"}},[e._v("$")]),e._v(" expunge")]),e._v(" "),a("p",[e._v("清空回收站：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("$ hdfs dfs -expunge\n16/11/07 01:55:54 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n")])])]),a("h3",{attrs:{id:"setrep"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#setrep"}},[e._v("$")]),e._v(" setrep")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Usage: hadoop fs [generic options] -setrep [-R] [-w] <rep> <path> ...\n")])])]),a("h3",{attrs:{id:"distcp"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#distcp"}},[e._v("$")]),e._v(" distcp")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("usage: distcp OPTIONS [source_path...] <target_path>\n              OPTIONS\n -append                       Reuse existing data in target files and\n                               append new data to them if possible\n -async                        Should distcp execution be blocking\n -atomic                       Commit all changes or none\n -bandwidth <arg>              Specify bandwidth per map in MB\n -blocksperchunk <arg>         If set to a positive value, fileswith more\n                               blocks than this value will be split into\n                               chunks of <blocksperchunk> blocks to be\n                               transferred in parallel, and reassembled on\n                               the destination. By default,\n                               <blocksperchunk> is 0 and the files will be\n                               transmitted in their entirety without\n                               splitting. This switch is only applicable\n                               when the source file system implements\n                               getBlockLocations method and the target\n                               file system implements concat method\n -copybuffersize <arg>         Size of the copy buffer to use. By default\n                               <copybuffersize> is 8192B.\n -delete                       Delete from target, files missing in source\n -diff <arg>                   Use snapshot diff report to identify the\n                               difference between source and target\n -f <arg>                      List of files that need to be copied\n -filelimit <arg>              (Deprecated!) Limit number of files copied\n                               to <= n\n -filters <arg>                The path to a file containing a list of\n                               strings for paths to be excluded from the\n                               copy.\n -i                            Ignore failures during copy\n -log <arg>                    Folder on DFS where distcp execution logs\n                               are saved\n -m <arg>                      Max number of concurrent maps to use for\n                               copy\n -mapredSslConf <arg>          Configuration for ssl config file, to use\n                               with hftps://. Must be in the classpath.\n -numListstatusThreads <arg>   Number of threads to use for building file\n                               listing (max 40).\n -overwrite                    Choose to overwrite target files\n                               unconditionally, even if they exist.\n -p <arg>                      preserve status (rbugpcaxt)(replication,\n                               block-size, user, group, permission,\n                               checksum-type, ACL, XATTR, timestamps). If\n                               -p is specified with no <arg>, then\n                               preserves replication, block size, user,\n                               group, permission, checksum type and\n                               timestamps. raw.* xattrs are preserved when\n                               both the source and destination paths are\n                               in the /.reserved/raw hierarchy (HDFS\n                               only). raw.* xattrpreservation is\n                               independent of the -p flag. Refer to the\n                               DistCp documentation for more details.\n -rdiff <arg>                  Use target snapshot diff report to identify\n                               changes made on target\n -sizelimit <arg>              (Deprecated!) Limit number of files copied\n                               to <= n bytes\n -skipcrccheck                 Whether to skip CRC checks between source\n                               and target paths.\n -strategy <arg>               Copy strategy to use. Default is dividing\n                               work based on file sizes\n -tmp <arg>                    Intermediate work path to be used for\n                               atomic commit\n -update                       Update target, copying only missingfiles or\n                               directories\n")])])]),a("h2",{attrs:{id:"参考"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[e._v("$")]),e._v(" 参考")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://stackoverflow.com/questions/39103872/merging-small-files-in-hadoop?answertab=votes#tab-top",target:"_blank",rel:"noopener noreferrer"}},[e._v("Merging small files in hadoop"),a("OutboundLink")],1)]),e._v(" "),a("p",[a("a",{attrs:{href:"https://linoxide.com/file-system/hadoop-hdfs-shell-commands/",target:"_blank",rel:"noopener noreferrer"}},[e._v("30 Most Frequently Used Hadoop HDFS Shell Commands"),a("OutboundLink")],1)])])}),[],!1,null,null,null);a.default=r.exports}}]);