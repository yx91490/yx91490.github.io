(window.webpackJsonp=window.webpackJsonp||[]).push([[52],{438:function(e,a,t){"use strict";t.r(a);var o=t(10),n=Object(o.a)({},(function(){var e=this,a=e._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"distcp命令"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#distcp命令"}},[e._v("$")]),e._v(" Distcp命令")]),e._v(" "),a("p",[e._v("usage：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("usage: distcp OPTIONS [source_path...] <target_path>\n                            OPTIONS\n -append                       Reuse existing data in target files and\n                               append new data to them if possible\n -async                        Should distcp execution be blocking\n -atomic                       Commit all changes or none\n -bandwidth <arg>              Specify bandwidth per map in MB\n -blocksperchunk <arg>         If set to a positive value, fileswith more\n                               blocks than this value will be split into\n                               chunks of <blocksperchunk> blocks to be\n                               transferred in parallel, and reassembled on\n                               the destination. By default,\n                               <blocksperchunk> is 0 and the files will be\n                               transmitted in their entirety without\n                               splitting. This switch is only applicable\n                               when the source file system implements\n                               getBlockLocations method and the target\n                               file system implements concat method\n -copybuffersize <arg>         Size of the copy buffer to use. By default\n                               <copybuffersize> is 8192B.\n -delete                       Delete from target, files missing in source\n -diff <arg>                   Use snapshot diff report to identify the\n                               difference between source and target\n -f <arg>                      List of files that need to be copied\n -filelimit <arg>              (Deprecated!) Limit number of files copied\n                               to <= n\n -filters <arg>                The path to a file containing a list of\n                               strings for paths to be excluded from the\n                               copy.\n -i                            Ignore failures during copy\n -log <arg>                    Folder on DFS where distcp execution logs\n                               are saved\n -m <arg>                      Max number of concurrent maps to use for\n                               copy\n -mapredSslConf <arg>          Configuration for ssl config file, to use\n                               with hftps://. Must be in the classpath.\n -numListstatusThreads <arg>   Number of threads to use for building file\n                               listing (max 40).\n -overwrite                    Choose to overwrite target files\n                               unconditionally, even if they exist.\n -p <arg>                      preserve status (rbugpcaxt)(replication,\n                               block-size, user, group, permission,\n                               checksum-type, ACL, XATTR, timestamps). If\n                               -p is specified with no <arg>, then\n                               preserves replication, block size, user,\n                               group, permission, checksum type and\n                               timestamps. raw.* xattrs are preserved when\n                               both the source and destination paths are\n                               in the /.reserved/raw hierarchy (HDFS\n                               only). raw.* xattrpreservation is\n                               independent of the -p flag. Refer to the\n                               DistCp documentation for more details.\n -rdiff <arg>                  Use target snapshot diff report to identify\n                               changes made on target\n -sizelimit <arg>              (Deprecated!) Limit number of files copied\n                               to <= n bytes\n -skipcrccheck                 Whether to skip CRC checks between source\n                               and target paths.\n -strategy <arg>               Copy strategy to use. Default is dividing\n                               work based on file sizes\n -tmp <arg>                    Intermediate work path to be used for\n                               atomic commit\n -update                       Update target, copying only missingfiles or\n                               directories\n")])])]),a("h3",{attrs:{id:"注意事项"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#注意事项"}},[e._v("$")]),e._v(" 注意事项")]),e._v(" "),a("ol",[a("li",[e._v("使用有权限的用户（hdfs）")]),e._v(" "),a("li",[e._v("在源集群运行命令")]),e._v(" "),a("li",[e._v("目的集群使用active namenode地址。")])]),e._v(" "),a("p",[a("strong",[e._v("尝试一")])]),e._v(" "),a("p",[e._v("命令：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("hadoop distcp -atomic hdfs://nameservice1/user/58box/ hdfs://10.9.15.227:9000/home/hdp_mis_yxzn/resultdata/58box/online\n")])])]),a("p",[e._v("报错：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("Caused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://nameservice1/user/58box/2011010815591950f73e6a/1055776356865953793 to hdfs://10.9.15.227:9000/home/hdp_mis_yxzn/resultdata/58box/._WIP_online-342047731/2011010815591950f73e6a/1055776356865953793\n\tat org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)\n\tat org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:304)\n\t... 11 more\nCaused by: java.io.IOException: Check-sum mismatch between hdfs://nameservice1/user/58box/2011010815591950f73e6a/1055776356865953793 and hdfs://10.9.15.227:9000/home/hdp_mis_yxzn/resultdata/58box/._WIP_online-342047731/.distcp.tmp.attempt_local1251455226_0001_m_000000_0. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:221)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:134)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)\n\tat org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)\n\t... 12 more\n")])])]),a("p",[a("strong",[e._v("尝试二")])]),e._v(" "),a("p",[e._v("命令：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("hadoop distcp -pb -atomic -log /tmp/box_distcp.log hdfs://nameservice1/user/58box/ hdfs://10.9.15.227:9000/home/hdp_mis_yxzn/resultdata/58box/online\n")])])]),a("p",[e._v("报错：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "tjtx148-14-137.58os.org/10.148.14.137"; destination host is: "bjm6-15-227.58os.org":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1508)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1441)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n\tat com.sun.proxy.$Proxy10.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:425)\n\tat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n\tat com.sun.proxy.$Proxy11.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1860)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1656)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:790)\nCaused by: java.io.IOException: Connection reset by peer\n\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n\tat sun.nio.ch.IOUtil.read(IOUtil.java:197)\n\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380)\n\tat org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:553)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1113)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1006)\n19/03/27 16:51:33 ERROR util.RetriableCommand: Failure in Retriable command: Copying hdfs://nameservice1/user/58box/2011010815591950f73e6a/1055384719614566400 to hdfs://10.9.15.227:9000/home/hdp_mis_yxzn/resultdata/58box/._WIP_online-1811973567/2011010815591950f73e6a/1055384719614566400\njava.net.ConnectException: Call From tjtx148-14-137.58os.org/10.148.14.137 to bjm6-15-227.58os.org:9000 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1508)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1441)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\n\tat com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:788)\n\tat sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:258)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)\n\tat com.sun.proxy.$Proxy11.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2168)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1266)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1262)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1262)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1418)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:148)\n\tat org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)\n\tat org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)\n\tat org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:304)\n\tat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:267)\n\tat org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:52)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:270)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.ConnectException: 拒绝连接\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:648)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:744)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3000(Client.java:396)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1557)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1480)\n\t... 31 more\n')])])]),a("blockquote",[a("p",[a("a",{attrs:{href:"https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_admin_distcp_data_cluster_migrate.html#concept_yjz_2wn_bbb",target:"_blank",rel:"noopener noreferrer"}},[e._v("Using DistCp with Highly Available Remote Clusters"),a("OutboundLink")],1)])]),e._v(" "),a("ol",[a("li",[a("p",[e._v("新建一个distcpConf目录，把本集群的/etc/hadoop/conf内容拷贝到里面")])]),e._v(" "),a("li",[a("p",[e._v("修改distcpConf/hdfs-site.xml，把远程集群的nameservice ID加入dfs.nameservices属性中，如果nameservice一样则重新命名：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("<property>\n <name>dfs.nameservices</name>\n <value>nameservice1,externalnameservice</value>\n </property>\n")])])])]),e._v(" "),a("li",[a("p",[e._v("把远程集群hdfs-site.xml中相关属性加入到distcpConf/hdfs-site.xml中，nameserviceID要和第二步中的保持一致：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("dfs.ha.namenodes.<nameserviceID>\ndfs.client.failover.proxy.provider.<remote nameserviceID>\ndfs.ha.automatic-failover.enabled.<remote nameserviceID>\ndfs.namenode.rpc-address.<nameserviceID>.<namenode1>\ndfs.namenode.servicerpc-address.<nameserviceID>.<namenode1>\ndfs.namenode.http-address.<nameserviceID>.<namenode1>\ndfs.namenode.https-address.<nameserviceID>.<namenode1>\ndfs.namenode.rpc-address.<nameserviceID>.<namenode2>\ndfs.namenode.servicerpc-address.<nameserviceID>.<namenode2>\ndfs.namenode.http-address.<nameserviceID>.<namenode2>\ndfs.namenode.https-address.<nameserviceID>.<namenode2>\n")])])])]),e._v(" "),a("li",[a("p",[e._v("运行distcp命令：")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("#不安全集群\nhadoop --config distcpConf distcp hdfs://<nameservice>/<source_directory> <target directory>\n#安全集群\nhadoop --config distcpConf distcp -Dmapreduce.job.hdfs-servers.token-renewal.exclude=<nameservice>  hdfs://<nameservice>/<source_directory> <target directory>\n")])])])])]),e._v(" "),a("blockquote",[a("p",[a("a",{attrs:{href:"https://hadoop.apache.org/docs/current/hadoop-distcp/DistCp.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("DistCp Version2 Guide"),a("OutboundLink")],1)])])])}),[],!1,null,null,null);a.default=n.exports}}]);