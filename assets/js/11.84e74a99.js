(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{325:function(t,e,a){t.exports=a.p+"assets/img/spark_deps.9c9e8740.png"},326:function(t,e,a){t.exports=a.p+"assets/img/1228818-20180426212247280-1642194580.d189c3da.png"},327:function(t,e,a){t.exports=a.p+"assets/img/1228818-20180426212601088-1983526282-20200813090739016.53e33318.png"},328:function(t,e,a){t.exports=a.p+"assets/img/1228818-20180426212726300-1935303266.2e0eb3f2.png"},329:function(t,e,a){t.exports=a.p+"assets/img/1228818-20180426212753315-871591593.d53c0406.png"},471:function(t,e,a){"use strict";a.r(e);var r=a(10),s=Object(r.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"spark"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#spark"}},[t._v("$")]),t._v(" Spark")]),t._v(" "),e("h2",{attrs:{id:"安装部署"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#安装部署"}},[t._v("$")]),t._v(" 安装部署")]),t._v(" "),e("p",[t._v("安装"),e("code",[t._v("spark-2.4.0-bin-without-hadoop.tgz")]),t._v(" 与"),e("code",[t._v("hadoop3.1.1")]),t._v("：")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v('Error: A JNI error has occurred, please check your installation and try again\nException in thread "main" java.lang.NoClassDefFoundError: org/slf4j/Logger\n\tat java.lang.Class.getDeclaredMethods0(Native Method)\n\tat java.lang.Class.privateGetDeclaredMethods(Class.java:2701)\n\tat java.lang.Class.privateGetMethodRecursive(Class.java:3048)\n\tat java.lang.Class.getMethod0(Class.java:3018)\n\tat java.lang.Class.getMethod(Class.java:1784)\n\tat sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:650)\n\tat sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:632)\nCaused by: java.lang.ClassNotFoundException: org.slf4j.Logger\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n\t... 7 more\n')])])]),e("p",[t._v("解决方法：")]),t._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("echo")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"export SPARK_DIST_CLASSPATH='),e("span",{pre:!0,attrs:{class:"token variable"}},[e("span",{pre:!0,attrs:{class:"token variable"}},[t._v("$(")]),t._v("hadoop classpath"),e("span",{pre:!0,attrs:{class:"token variable"}},[t._v(")")])]),t._v('"')]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">>")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token variable"}},[t._v("${SPARK_HOME}")]),t._v("/conf/spark-env.sh\n")])])]),e("h2",{attrs:{id:"应用提交"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#应用提交"}},[t._v("$")]),t._v(" 应用提交")]),t._v(" "),e("p",[t._v("Master URL有效格式：")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Master URL")]),t._v(" "),e("th",[t._v("含义")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("local")]),t._v(" "),e("td",[t._v("使用单线程本地运行Spark")])]),t._v(" "),e("tr",[e("td",[t._v("local[K]")]),t._v(" "),e("td",[t._v("使用K个线程本地运行Spark")])]),t._v(" "),e("tr",[e("td",[t._v("local[K,F]")]),t._v(" "),e("td",[t._v("本地运行Spark，使用K个线程，最多允许F个线程失败")])]),t._v(" "),e("tr",[e("td",[t._v("local[*]")]),t._v(" "),e("td",[t._v("使用与机器逻辑核心数相同的线程数本地运行Spark")])]),t._v(" "),e("tr",[e("td",[t._v("local[*,F]")]),t._v(" "),e("td",[t._v("本地运行Spark，使用与机器逻辑核心数相同的线程数，最多允许F个线程失败")])]),t._v(" "),e("tr",[e("td",[t._v("spark://HOST:PORT")]),t._v(" "),e("td",[t._v("连接到Spark 独立集群的master节点")])]),t._v(" "),e("tr",[e("td",[t._v("spark://HOST1:PORT1,HOST2:PORT2")]),t._v(" "),e("td",[t._v("连接到有备用master的spark独立集群，连接列表必须包含用zookeeper搭建的高可用集群中的所有master地址，端口号默认为7077")])]),t._v(" "),e("tr",[e("td",[t._v("mesos://HOST:PORT")]),t._v(" "),e("td",[t._v("连接到mesos集群，端口号默认为5050。对于使用zk的mesos集群，使用mesos://zk://格式的URL。使用"),e("code",[t._v("--deploy-mode cluster")]),t._v("提交的话，HOST:PORT应该配置成连接到MesosClusterDispatcher")])]),t._v(" "),e("tr",[e("td",[t._v("yarn")]),t._v(" "),e("td",[t._v("根据"),e("code",[t._v("--deploy-mode")]),t._v("的值不同以client或者cluster模式连接到一个YARN集群，集群地址来源于"),e("code",[t._v("HADOOP_CONF_DIR")]),t._v("或者"),e("code",[t._v("YARN_CONF_DIR")]),t._v("变量")])]),t._v(" "),e("tr",[e("td",[t._v("k8s://HOST:PORT")]),t._v(" "),e("td",[t._v("以cluster模式连接到一个kubernetes集群，未来会支持client模式。HOST 和 PORT 参考 "),e("a",{attrs:{href:"https://kubernetes.io/docs/reference/generated/kube-apiserver/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kubernetes API Server"),e("OutboundLink")],1),t._v(". 默认使用TLS连接。为了强制使用不安全的连接，可以使用格式：k8s://http://HOST:PORT.")])])])]),t._v(" "),e("p",[t._v("参考："),e("a",{attrs:{href:"https://spark.apache.org/docs/2.4.0/submitting-applications.html#master-urls",target:"_blank",rel:"noopener noreferrer"}},[t._v("Master URLs"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"rdd编程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#rdd编程"}},[t._v("$")]),t._v(" RDD编程")]),t._v(" "),e("h4",{attrs:{id:"窄依赖和宽依赖"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#窄依赖和宽依赖"}},[t._v("$")]),t._v(" 窄依赖和宽依赖")]),t._v(" "),e("p",[e("img",{attrs:{src:a(325),alt:"spark_deps"}})]),t._v(" "),e("p",[t._v("每个 Transformation 操作都会生成一个新的 RDD，RDD 和它依赖的父 RDD（s）的关系有两种不同的类型，即窄依赖和宽依赖：")]),t._v(" "),e("ul",[e("li",[t._v("窄依赖指的是子 RDD 只依赖于父 RDD 中一个固定数量的分区")]),t._v(" "),e("li",[t._v("宽依赖指的是子 RDD 的每一个分区都依赖于父 RDD 的所有分区")])]),t._v(" "),e("p",[t._v("Spark 会将每一个 Job 分为多个不同的 Stage, 而 Stage 之间的依赖关系则形成了有向无环图。")]),t._v(" "),e("p",[t._v("Spark 会根据 RDD 之间的依赖关系将 DAG 图划分为不同的阶段。")]),t._v(" "),e("p",[t._v("对于窄依赖，由于 Partition 依赖关系的确定性，Partition 的转换处理就可以在同一个线程里完成，窄依赖就被 Spark 划分到同一个 stage 中。")]),t._v(" "),e("p",[t._v("而对于宽依赖，只能等父RDD shuffle 处理完成后，下一个 stage 才能开始接下来的计算。")]),t._v(" "),e("h4",{attrs:{id:"划分stage过程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#划分stage过程"}},[t._v("$")]),t._v(" 划分Stage过程")]),t._v(" "),e("ul",[e("li",[t._v("首先根据rdd的算子操作顺序生成DAG有向无环图，接下里从最后一个rdd往前推，创建一个新的stage，把该rdd加入到该stage中，它是最后一个stage。")]),t._v(" "),e("li",[t._v("在往前推的过程中运行遇到了窄依赖就把该rdd加入到本stage中，如果遇到了宽依赖，就从宽依赖切开，那么最后一个stage也就结束了。")]),t._v(" "),e("li",[t._v("重新创建一个新的stage，按照第二个步骤继续往前推，一直到最开始的rdd，整个划分stage也就结束了")])]),t._v(" "),e("h4",{attrs:{id:"设置并行度"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#设置并行度"}},[t._v("$")]),t._v(" 设置并行度")]),t._v(" "),e("p",[t._v("// TODO")]),t._v(" "),e("h3",{attrs:{id:"transformations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformations"}},[t._v("$")]),t._v(" Transformations")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("Transformation")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("Meaning")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("map")]),t._v("("),e("em",[t._v("func")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return a new distributed dataset formed by passing each element of the source through a function "),e("em",[t._v("func")]),t._v(".")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("filter")]),t._v("("),e("em",[t._v("func")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return a new dataset formed by selecting those elements of the source on which "),e("em",[t._v("func")]),t._v(" returns true.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("flatMap")]),t._v("("),e("em",[t._v("func")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Similar to map, but each input item can be mapped to 0 or more output items (so "),e("em",[t._v("func")]),t._v(" should return a Seq rather than a single item).")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("mapPartitions")]),t._v("("),e("em",[t._v("func")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Similar to map, but runs separately on each partition (block) of the RDD, so "),e("em",[t._v("func")]),t._v(" must be of type Iterator<T> => Iterator<U> when running on an RDD of type T.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("mapPartitionsWithIndex")]),t._v("("),e("em",[t._v("func")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Similar to mapPartitions, but also provides "),e("em",[t._v("func")]),t._v(" with an integer value representing the index of the partition, so "),e("em",[t._v("func")]),t._v(" must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("sample")]),t._v("("),e("em",[t._v("withReplacement")]),t._v(", "),e("em",[t._v("fraction")]),t._v(", "),e("em",[t._v("seed")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Sample a fraction "),e("em",[t._v("fraction")]),t._v(" of the data, with or without replacement, using a given random number generator seed.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("union")]),t._v("("),e("em",[t._v("otherDataset")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return a new dataset that contains the union of the elements in the source dataset and the argument.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("intersection")]),t._v("("),e("em",[t._v("otherDataset")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return a new RDD that contains the intersection of elements in the source dataset and the argument.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("distinct")]),t._v("(["),e("em",[t._v("numPartitions")]),t._v("]))")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return a new dataset that contains the distinct elements of the source dataset.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("groupByKey")]),t._v("(["),e("em",[t._v("numPartitions")]),t._v("])")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. "),e("strong",[t._v("Note:")]),t._v(" If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using "),e("code",[t._v("reduceByKey")]),t._v(" or "),e("code",[t._v("aggregateByKey")]),t._v(" will yield much better performance. "),e("strong",[t._v("Note:")]),t._v(" By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional "),e("code",[t._v("numPartitions")]),t._v(" argument to set a different number of tasks.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("reduceByKey")]),t._v("("),e("em",[t._v("func")]),t._v(", ["),e("em",[t._v("numPartitions")]),t._v("])")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function "),e("em",[t._v("func")]),t._v(", which must be of type (V,V) => V. Like in "),e("code",[t._v("groupByKey")]),t._v(", the number of reduce tasks is configurable through an optional second argument.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("aggregateByKey")]),t._v("("),e("em",[t._v("zeroValue")]),t._v(")("),e("em",[t._v("seqOp")]),t._v(", "),e("em",[t._v("combOp")]),t._v(", ["),e("em",[t._v("numPartitions")]),t._v("])")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v('When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in '),e("code",[t._v("groupByKey")]),t._v(", the number of reduce tasks is configurable through an optional second argument.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("sortByKey")]),t._v("(["),e("em",[t._v("ascending")]),t._v("], ["),e("em",[t._v("numPartitions")]),t._v("])")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean "),e("code",[t._v("ascending")]),t._v(" argument.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("join")]),t._v("("),e("em",[t._v("otherDataset")]),t._v(", ["),e("em",[t._v("numPartitions")]),t._v("])")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through "),e("code",[t._v("leftOuterJoin")]),t._v(", "),e("code",[t._v("rightOuterJoin")]),t._v(", and "),e("code",[t._v("fullOuterJoin")]),t._v(".")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("cogroup")]),t._v("("),e("em",[t._v("otherDataset")]),t._v(", ["),e("em",[t._v("numPartitions")]),t._v("])")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called "),e("code",[t._v("groupWith")]),t._v(".")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("cartesian")]),t._v("("),e("em",[t._v("otherDataset")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("pipe")]),t._v("("),e("em",[t._v("command")]),t._v(", "),e("em",[t._v("[envVars]")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("coalesce")]),t._v("("),e("em",[t._v("numPartitions")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("repartition")]),t._v("("),e("em",[t._v("numPartitions")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("repartitionAndSortWithinPartitions")]),t._v("("),e("em",[t._v("partitioner")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling "),e("code",[t._v("repartition")]),t._v(" and then sorting within each partition because it can push the sorting down into the shuffle machinery.")])])])]),t._v(" "),e("p",[t._v("其中shuffle算子包括：repartition操作如repartition 和 coalesce，ByKey操作（除了countByKey）如groupByKey 和 reduceByKey，join操作如cogroup 和 join。")]),t._v(" "),e("h3",{attrs:{id:"actions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#actions"}},[t._v("$")]),t._v(" Actions")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("Action")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("Meaning")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("reduce")]),t._v("("),e("em",[t._v("func")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Aggregate the elements of the dataset using a function "),e("em",[t._v("func")]),t._v(" (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("collect")]),t._v("()")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("count")]),t._v("()")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return the number of elements in the dataset.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("first")]),t._v("()")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return the first element of the dataset (similar to take(1)).")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("take")]),t._v("("),e("em",[t._v("n")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return an array with the first "),e("em",[t._v("n")]),t._v(" elements of the dataset.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("takeSample")]),t._v("("),e("em",[t._v("withReplacement")]),t._v(", "),e("em",[t._v("num")]),t._v(", ["),e("em",[t._v("seed")]),t._v("])")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return an array with a random sample of "),e("em",[t._v("num")]),t._v(" elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("takeOrdered")]),t._v("("),e("em",[t._v("n")]),t._v(", "),e("em",[t._v("[ordering]")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Return the first "),e("em",[t._v("n")]),t._v(" elements of the RDD using either their natural order or a custom comparator.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("saveAsTextFile")]),t._v("("),e("em",[t._v("path")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("saveAsSequenceFile")]),t._v("("),e("em",[t._v("path")]),t._v(") (Java and Scala)")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("saveAsObjectFile")]),t._v("("),e("em",[t._v("path")]),t._v(") (Java and Scala)")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using "),e("code",[t._v("SparkContext.objectFile()")]),t._v(".")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("countByKey")]),t._v("()")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[e("strong",[t._v("foreach")]),t._v("("),e("em",[t._v("func")]),t._v(")")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("Run a function "),e("em",[t._v("func")]),t._v(" on each element of the dataset. This is usually done for side effects such as updating an "),e("a",{attrs:{href:"https://spark.apache.org/docs/2.4.0/rdd-programming-guide.html#accumulators",target:"_blank",rel:"noopener noreferrer"}},[t._v("Accumulator"),e("OutboundLink")],1),t._v(" or interacting with external storage systems. "),e("strong",[t._v("Note")]),t._v(": modifying variables other than Accumulators outside of the "),e("code",[t._v("foreach()")]),t._v(" may result in undefined behavior. See "),e("a",{attrs:{href:"https://spark.apache.org/docs/2.4.0/rdd-programming-guide.html#understanding-closures-a-nameclosureslinka",target:"_blank",rel:"noopener noreferrer"}},[t._v("Understanding closures "),e("OutboundLink")],1),t._v("for more details.")])])])]),t._v(" "),e("h3",{attrs:{id:"参考"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[t._v("$")]),t._v(" 参考")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://spark.apache.org/docs/2.4.0/rdd-programming-guide.html#transformations",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformations"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://spark.apache.org/docs/2.4.0/rdd-programming-guide.html#actions",target:"_blank",rel:"noopener noreferrer"}},[t._v("Actions"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://medium.com/@dvcanton/wide-and-narrow-dependencies-in-apache-spark-21acf2faf031",target:"_blank",rel:"noopener noreferrer"}},[t._v("Wide and Narrow dependencies in Apache Spark"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.zhihu.com/question/37137360",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark中的narrow/wide dependency如何理解，有什么作用?"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/JerryLead/SparkInternals/blob/master/markdown/2-JobLogicalPlan.md",target:"_blank",rel:"noopener noreferrer"}},[t._v("Job 逻辑执行图"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://whiteding.fun/2020/05/26/Spark%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%EF%BC%9A%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AE%E5%B9%B6%E8%A1%8C%E5%BA%A6/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark性能调优：合理设置并行度"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"配置"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#配置"}},[t._v("$")]),t._v(" 配置")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("属性名")]),t._v(" "),e("th",[t._v("默认值")]),t._v(" "),e("th",[t._v("含义")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("spark.default.parallelism")]),t._v(" "),e("td",[t._v("对于像是"),e("code",[t._v("reduceByKey")]),t._v("、 "),e("code",[t._v("join")]),t._v("的分布式shuffle操作，取值为父RDD最大的分区数，对于像"),e("code",[t._v("parallelize")]),t._v("这样的没有父RDD的操作，它依赖于cluster manager："),e("br"),t._v("Local模式：本机的核数"),e("br"),t._v("Mesos fine grained mode: 8"),e("br"),t._v("其他：2和所有executor节点核数的最大值")]),t._v(" "),e("td",[t._v("RDD中transformations算子默认返回的分区数")])]),t._v(" "),e("tr",[e("td"),t._v(" "),e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td"),t._v(" "),e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td"),t._v(" "),e("td"),t._v(" "),e("td")])])]),t._v(" "),e("h3",{attrs:{id:"参考-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考-2"}},[t._v("$")]),t._v(" 参考")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://spark.apache.org/docs/2.4.0/configuration.html#execution-behavior",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark Configuration"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/97700916",target:"_blank",rel:"noopener noreferrer"}},[t._v("通过spark.default.parallelism谈Spark并行度"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"代码示例"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#代码示例"}},[t._v("$")]),t._v(" 代码示例")]),t._v(" "),e("h3",{attrs:{id:"参考-3"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考-3"}},[t._v("$")]),t._v(" 参考")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples",target:"_blank",rel:"noopener noreferrer"}},[t._v("examples"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"源码编译"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#源码编译"}},[t._v("$")]),t._v(" 源码编译")]),t._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[t._v("./dev/make-distribution.sh "),e("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--name")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.6")]),t._v(".0 "),e("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("--tgz")]),t._v("  "),e("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("-Pyarn")]),t._v(" -Phadoop-2.6 "),e("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("-Phive")]),t._v(" -Phive-thriftserver "),e("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("-Dhadoop.version")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.6")]),t._v(".0 "),e("span",{pre:!0,attrs:{class:"token parameter variable"}},[t._v("-X")]),t._v("\n")])])]),e("p",[t._v("spark-shell测试：")]),t._v(" "),e("div",{staticClass:"language-Scala extra-class"},[e("pre",{pre:!0,attrs:{class:"language-scala"}},[e("code",[t._v("sqlContext"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"select * from ods.tab1"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),e("p",[t._v("spark2-shell测试：")]),t._v(" "),e("div",{staticClass:"language-scala extra-class"},[e("pre",{pre:!0,attrs:{class:"language-scala"}},[e("code",[t._v("spark"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sql"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"select * from ods.tab1"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),e("h2",{attrs:{id:"性能调优"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#性能调优"}},[t._v("$")]),t._v(" 性能调优")]),t._v(" "),e("p",[t._v("增加executor数 => 增加并行执行的能力")]),t._v(" "),e("p",[t._v("增加每个executor的cpu core数 => 增加并行执行的能力")]),t._v(" "),e("p",[t._v("增加每个executor的内存量：")]),t._v(" "),e("ul",[e("li",[t._v("如果需要对RDD进行缓存 => 可以缓存更多数据 => 减少磁盘IO")]),t._v(" "),e("li",[t._v("对于shuffle操作，reducer需要内存存放数据和聚合 => 减少磁盘IO")]),t._v(" "),e("li",[t._v("减少Task的GC时间")])]),t._v(" "),e("p",[t._v("并行度：Spark作业中各个stage的task数量：")]),t._v(" "),e("ul",[e("li",[t._v("至少设置成与Spark application的总cpu core数量相同")]),t._v(" "),e("li",[t._v("官方是推荐设置成spark application总cpu core数量的2~3倍（尽量让cpu core不要空闲，同时也是尽量提升spark作业运行的效率和速度，提升性能。）")])]),t._v(" "),e("p",[t._v("如何设置一个Spark Application的并行度？")]),t._v(" "),e("p",[t._v("设置"),e("code",[t._v("spark.default.parallelism")]),t._v("。")]),t._v(" "),e("h4",{attrs:{id:"rdd持久化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#rdd持久化"}},[t._v("$")]),t._v(" RDD持久化")]),t._v(" "),e("p",[t._v("尽量去复用RDD。")]),t._v(" "),e("p",[t._v("公共RDD一定要实现持久化：即将RDD的数据缓存到内存中/磁盘中，（BlockManager），以后无论对这个RDD做多少次计算，那么都是直接取这个RDD的持久化的数据，比如从内存中或者磁盘中，直接提取一份数据。")]),t._v(" "),e("p",[t._v("将RDD序列化后可以大大减少内存的空间占用，序列化的唯一的缺点就是在获取数据的时候需要反序列化。")]),t._v(" "),e("p",[t._v("在内存资源极度充足的情况下为了数据的高可靠性，可以使用双副本机制进行持久化。")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",{staticStyle:{"text-align":"left"}},[t._v("存储级别")]),t._v(" "),e("th",{staticStyle:{"text-align":"left"}},[t._v("意义")])])]),t._v(" "),e("tbody",[e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("MEMORY_ONLY")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("将 RDD 作为反序列化的 Java 对象存储在 JVM 中。如果 RDD 不适合内存，某些分区将不会被缓存，并且会在每次需要时重新计算。这是默认级别。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("MEMORY_AND_DISK")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("将 RDD 作为反序列化的 Java 对象存储在 JVM 中。如果 RDD 不适合内存，则存储不适合磁盘的分区，并在需要时从那里读取它们。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("MEMORY_ONLY_SER （Java 和 Scala）")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("将 RDD 存储为"),e("em",[t._v("序列化的")]),t._v("Java 对象（每个分区一个字节数组）。这通常比反序列化对象更节省空间，尤其是在使用"),e("a",{attrs:{href:"https://spark.apache.org/docs/2.4.0/tuning.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("快速序列化器时"),e("OutboundLink")],1),t._v("，但读取时更占用 CPU。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("MEMORY_AND_DISK_SER （Java 和 Scala）")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("与 MEMORY_ONLY_SER 类似，但将不适合内存的分区溢出到磁盘，而不是在每次需要时即时重新计算它们。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("DISK_ONLY")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("仅将 RDD 分区存储在磁盘上。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("MEMORY_ONLY_2、MEMORY_AND_DISK_2 等")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("与上述级别相同，但在两个集群节点上复制每个分区。")])]),t._v(" "),e("tr",[e("td",{staticStyle:{"text-align":"left"}},[t._v("OFF_HEAP（实验性）")]),t._v(" "),e("td",{staticStyle:{"text-align":"left"}},[t._v("与 MEMORY_ONLY_SER 类似，但将数据存储在 "),e("a",{attrs:{href:"https://spark.apache.org/docs/2.4.0/configuration.html#memory-management",target:"_blank",rel:"noopener noreferrer"}},[t._v("堆外内存中"),e("OutboundLink")],1),t._v("。这需要启用堆外内存。")])])])]),t._v(" "),e("p",[t._v("参考："),e("a",{attrs:{href:"https://spark.apache.org/docs/2.4.0/rdd-programming-guide.html#rdd-persistence",target:"_blank",rel:"noopener noreferrer"}},[t._v("RDD Persistence"),e("OutboundLink")],1)]),t._v(" "),e("h4",{attrs:{id:"广播变量"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#广播变量"}},[t._v("$")]),t._v(" 广播变量")]),t._v(" "),e("p",[t._v("广播变量：初始的时候，只在Drvier上有一份副本（不是每个task一份副本，而是每个executor才一份副本，这样可以让变量产生的副本大大减少）")]),t._v(" "),e("p",[t._v("task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中，尝试获取变量副本；如果本地没有，那么就从Driver远程拉取变量副本，并保存在本地的BlockManager中；此后这个executor上的task，都会直接使用本地的BlockManager中的副本。")]),t._v(" "),e("p",[t._v("executor的BlockManager除了从driver上拉取，也可能从其他节点的BlockManager上拉取变量副本，距离越近越好。")]),t._v(" "),e("h4",{attrs:{id:"使用kryo序列化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#使用kryo序列化"}},[t._v("$")]),t._v(" 使用Kryo序列化")]),t._v(" "),e("p",[t._v("设置"),e("code",[t._v("spark.serializer=org.apache.spark.serializer.KryoSerializer")])]),t._v(" "),e("p",[t._v("启用Kryo序列化机制会生效的地方：")]),t._v(" "),e("ol",[e("li",[e("p",[t._v("算子函数中使用到的外部变量：优化网络传输的性能，优化集群内存的占用和消耗")])]),t._v(" "),e("li",[e("p",[t._v("持久化RDD时进行序列化，"),e("code",[t._v("StorageLevel.MEMORY_ONLY_SER")]),t._v("：优化内存的占用和消耗")])]),t._v(" "),e("li",[e("p",[t._v("Shuffle (在进行stage间的task的shuffle操作时，节点与节点之间的task会互相大量通过网络拉取和传输文件，此时，这些数据既然通过网络传输，也是可能要序列化的，就会使用Kryo)：优化网络传输的性能")])])]),t._v(" "),e("h4",{attrs:{id:"使用fastutil集合"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#使用fastutil集合"}},[t._v("$")]),t._v(" 使用fastutil集合")]),t._v(" "),e("h4",{attrs:{id:"数据本地化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#数据本地化"}},[t._v("$")]),t._v(" 数据本地化")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("英文")]),t._v(" "),e("th",[t._v("中文")]),t._v(" "),e("th",[t._v("解释")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("PROCESS_LOCAL")]),t._v(" "),e("td",[t._v("进程本地化")]),t._v(" "),e("td",[t._v("数据在executor的BlockManager中，性能最好")])]),t._v(" "),e("tr",[e("td",[t._v("NODE_LOCAL")]),t._v(" "),e("td",[t._v("节点本地化")]),t._v(" "),e("td",[t._v("task在节点上某个executor中运行，数据位于同节点的HDFS block块；"),e("br"),t._v("或者数据和task在一个节点上的不同executor中；数据需要在进程间进行传输")])]),t._v(" "),e("tr",[e("td",[t._v("NO_PREF")]),t._v(" "),e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("RACK_LOCAL")]),t._v(" "),e("td",[t._v("机架本地化")]),t._v(" "),e("td",[t._v("数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输。")])]),t._v(" "),e("tr",[e("td",[t._v("ANY")]),t._v(" "),e("td"),t._v(" "),e("td",[t._v("数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差")])])])]),t._v(" "),e("p",[t._v("参数调节：")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("参数")]),t._v(" "),e("th",[t._v("默认值")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("spark.locality.wait")]),t._v(" "),e("td",[t._v("3s")])]),t._v(" "),e("tr",[e("td",[t._v("spark.locality.wait.process")]),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("spark.locality.wait.node")]),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("spark.locality.wait.rack")]),t._v(" "),e("td")])])]),t._v(" "),e("h4",{attrs:{id:"executor堆外内存"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#executor堆外内存"}},[t._v("$")]),t._v(" executor堆外内存")]),t._v(" "),e("p",[t._v("executor的堆外内存不太够用会报错shuffle file cannot find，executor、task lost，out of memory（内存溢出）；")]),t._v(" "),e("p",[t._v("executor在运行的过程中内存溢出，可能导致后续的stage的task在运行的时候，可能要从一些executor中去拉取shuffle map output文件，但是executor可能已经挂掉了，关联的block manager也没有了；所以可能会报shuffle output file not found；resubmitting task；executor lost；spark作业彻底崩溃。")]),t._v(" "),e("p",[t._v("增加executor堆外内存："),e("code",[t._v("--conf spark.yarn.executor.memoryOverhead=2048")])]),t._v(" "),e("h4",{attrs:{id:"增加连接等待时长"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#增加连接等待时长"}},[t._v("$")]),t._v(" 增加连接等待时长")]),t._v(" "),e("p",[t._v("spark默认的网络连接的超时时长，是60s；如果卡住60s都无法建立连接的话，那么就宣告失败了。")]),t._v(" "),e("p",[t._v("当出现"),e("code",[t._v("file not found，executor lost，task lost")]),t._v("时，很有可能是有那份数据的executor在jvm gc。")]),t._v(" "),e("p",[t._v("配置连接时长："),e("code",[t._v("--conf spark.core.connection.ack.wait.timeout=300")]),t._v("。")]),t._v(" "),e("h4",{attrs:{id:"shuffle调优"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#shuffle调优"}},[t._v("$")]),t._v(" shuffle调优")]),t._v(" "),e("h5",{attrs:{id:"合并map端输出文件"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#合并map端输出文件"}},[t._v("$")]),t._v(" 合并map端输出文件")]),t._v(" "),e("p",[t._v("配置"),e("code",[t._v("spark.shuffle.consolidateFiles=true")])]),t._v(" "),e("h5",{attrs:{id:"map端内存缓存与reduce端内存占比"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#map端内存缓存与reduce端内存占比"}},[t._v("$")]),t._v(" map端内存缓存与reduce端内存占比")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("参数")]),t._v(" "),e("th",[t._v("默认值")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("spark.shuffle.file.buffer")]),t._v(" "),e("td",[t._v("32kb")])]),t._v(" "),e("tr",[e("td",[t._v("spark.shuffle.memoryFraction")]),t._v(" "),e("td",[t._v("0.2")])])])]),t._v(" "),e("p",[t._v("默认情况下，shuffle的map task，输出到磁盘文件的时候会先写入每个task自己关联的一个内存缓冲区。默认大小是32kb。")]),t._v(" "),e("p",[t._v("当内存缓冲区满溢之后，才会进行spill操作溢写到磁盘文件中去。")]),t._v(" "),e("p",[t._v("在map task处理的数据量比较大的情况下可能会造成多次的map端往磁盘文件的spill溢写操作，发生大量的磁盘IO，从而降低性能。")]),t._v(" "),e("p",[t._v("reduce端聚合内存，如果数据量比较大，reduce task拉取过来的数据很多，那么就会频繁发生reduce端聚合内存不够用，频繁发生spill操作，溢写到磁盘上去。后面在进行聚合操作的时候，很可能会多次读取磁盘中的数据，进行聚合。默认不调优，在数据量比较大的情况下，可能频繁地发生reduce端的磁盘文件的读写。")]),t._v(" "),e("h5",{attrs:{id:"shufflemanager"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#shufflemanager"}},[t._v("$")]),t._v(" ShuffleManager")]),t._v(" "),e("p",[t._v("临时文件数量：")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("ShuffleManager")]),t._v(" "),e("th",[t._v("机制")]),t._v(" "),e("th",[t._v("数量")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("HashShuffleManager")]),t._v(" "),e("td",[t._v("普通机制")]),t._v(" "),e("td",[t._v("M（map task的个数）*R（reduce task的个数）")])]),t._v(" "),e("tr",[e("td",[t._v("HashShuffleManager")]),t._v(" "),e("td",[t._v("优化机制")]),t._v(" "),e("td",[t._v("C（core的个数）*R（Reduce的个数）")])]),t._v(" "),e("tr",[e("td",[t._v("SortShuffleManager")]),t._v(" "),e("td",[t._v("普通机制")]),t._v(" "),e("td",[t._v("2*M")])]),t._v(" "),e("tr",[e("td",[t._v("SortShuffleManager")]),t._v(" "),e("td",[t._v("bypass机制")]),t._v(" "),e("td",[t._v("没有排序：2*M")])])])]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("参数")]),t._v(" "),e("th",[t._v("选项")]),t._v(" "),e("th",[t._v("备注")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("spark.shuffle.manager")]),t._v(" "),e("td",[t._v("sort（默认）,hash,tungsten-sort")]),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("spark.shuffle.sort.bypassMergeThreshold")]),t._v(" "),e("td",[t._v("200")]),t._v(" "),e("td",[t._v("输出文件小于等于200的；最后只会将所有的输出文件合并为一份文件，并不会进行sort操作")])])])]),t._v(" "),e("p",[t._v("SortShuffleManager会对每个reduce task要处理的数据，进行排序。")]),t._v(" "),e("p",[t._v("SortShuffleManager一个task只会写入一个磁盘文件，不同reduce task的数据用offset来划分界定。")]),t._v(" "),e("p",[t._v("HashShuffleManager会创建多份磁盘文件")]),t._v(" "),e("p",[t._v("TungStenSortShuffleManager自己实现内存管理，性能上有很大的提升， 可以避免shuffle过程中产生的大量的OOM，GC等。")]),t._v(" "),e("h5",{attrs:{id:"mappartitions提升map类操作性能"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#mappartitions提升map类操作性能"}},[t._v("$")]),t._v(" MapPartitions提升Map类操作性能")]),t._v(" "),e("h5",{attrs:{id:"使用foreachpartition算子"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#使用foreachpartition算子"}},[t._v("$")]),t._v(" 使用foreachPartition算子")]),t._v(" "),e("p",[t._v("foreachPartition算子的好处：")]),t._v(" "),e("p",[t._v("1、对于我们写的function函数，就调用一次，一次传入一个partition所有的数据")]),t._v(" "),e("p",[t._v("2、主要创建或者获取一个数据库连接就可以")]),t._v(" "),e("p",[t._v("3、只要向数据库发送一次SQL语句和多组参数即可")]),t._v(" "),e("h5",{attrs:{id:"spark-default-parallelism"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#spark-default-parallelism"}},[t._v("$")]),t._v(" spark.default.parallelism")]),t._v(" "),e("p",[t._v("官方推荐指定为cpu core总数的2~3倍。")]),t._v(" "),e("p",[t._v("设置的这个并行度，在哪些情况下会生效？哪些情况下，不会生效？")]),t._v(" "),e("p",[t._v("如果没有使用Spark SQL（DataFrame），那么你整个spark application默认所有stage的并行度都是你设置的那个参数。（除非你使用coalesce算子缩减过partition数量）")]),t._v(" "),e("p",[t._v("如果使用了Spark SQL，Spark SQL自己会默认根据hive表对应的hdfs文件的block，自动设置Spark SQL查询所在的那个stage的并行度。")]),t._v(" "),e("p",[t._v("可以用于Spark SQL查询出来的RDD，使用repartition算子重新进行分区，此时可以分区成多个partition。然后从repartition以后的RDD，再往后并行度和task数量就会按照你预期的来了。就可以避免跟Spark SQL绑定在一个stage中的算子，只能使用少量的task去处理大量数据以及复杂的算法逻辑。")]),t._v(" "),e("h4",{attrs:{id:"控制shuffle-reduce端缓冲大小-避免oom"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#控制shuffle-reduce端缓冲大小-避免oom"}},[t._v("$")]),t._v(" 控制shuffle reduce端缓冲大小，避免OOM")]),t._v(" "),e("p",[t._v("配置"),e("code",[t._v("spark.reducer.maxSizeInFlight")])]),t._v(" "),e("p",[t._v("map端的task是不断的输出数据的，数据量可能是很大的。")]),t._v(" "),e("p",[t._v("而reduce端的task并不是等到map端task将属于自己的那份数据全部写入磁盘文件之后才去拉取的。map端写一点数据reduce端task就会拉取一小部分数据，立即进行后面的聚合、算子函数的应用。")]),t._v(" "),e("p",[t._v("每次reduece能够拉取多少数据就由buffer来决定。因为拉取过来的数据都是先放在buffer中的，然后才用后面的executor分配的堆内存占比（0.2），hashmap，去进行后续的聚合、函数的执行。")]),t._v(" "),e("p",[t._v("但是有的时候，map端的数据量特别大，然后写出的速度特别快。这时候就应该减少reduce端task缓冲的大小来避免OOM。")]),t._v(" "),e("h4",{attrs:{id:"解决jvm-gc导致的shuffle文件拉去失败问题"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#解决jvm-gc导致的shuffle文件拉去失败问题"}},[t._v("$")]),t._v(" 解决JVM GC导致的shuffle文件拉去失败问题")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("配置项")]),t._v(" "),e("th",[t._v("值")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("spark.shuffle.io.maxRetries")]),t._v(" "),e("td",[t._v("60")])]),t._v(" "),e("tr",[e("td",[t._v("spark.shuffle.io.retryWait")]),t._v(" "),e("td",[t._v("60s")])])])]),t._v(" "),e("h4",{attrs:{id:"解决各种序列化导致的报错"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#解决各种序列化导致的报错"}},[t._v("$")]),t._v(" 解决各种序列化导致的报错")]),t._v(" "),e("p",[t._v("用client模式去提交spark作业，观察本地打印出来的log。如果报错的log出现了类似于Serializable、Serialize等等字眼。")]),t._v(" "),e("p",[t._v("序列化报错要注意的3个点：")]),t._v(" "),e("ol",[e("li",[t._v("如果你的算子函数里面使用到了外部的自定义类型的变量，就要求你的自定义类型必须是可序列化的。")]),t._v(" "),e("li",[t._v("如果要将自定义的类型作为RDD的元素类型，那么自定义的类型也必须是可以序列化的")]),t._v(" "),e("li",[t._v("不能在上述两种情况下，去使用一些第三方的不支持序列化的类型（数据库连接Connection是不支持序列化的）")])]),t._v(" "),e("h4",{attrs:{id:"解决算子函数返回null导致的问题"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#解决算子函数返回null导致的问题"}},[t._v("$")]),t._v(" 解决算子函数返回NULL导致的问题")]),t._v(" "),e("p",[t._v("在有些算子函数里面是需要我们有一个返回值的。如果碰到你的确是对于某些值，不想要有返回值的话，有一个解决的办法：")]),t._v(" "),e("ol",[e("li",[t._v("在返回的时候，返回一些特殊的值")]),t._v(" "),e("li",[t._v("可以对这个RDD执行filter操作")]),t._v(" "),e("li",[t._v("在filter之后可以使用coalesce算子压缩一下RDD的partition的数量")])]),t._v(" "),e("h4",{attrs:{id:"解决yarn-client模式导致的网卡流量激增的问题"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#解决yarn-client模式导致的网卡流量激增的问题"}},[t._v("$")]),t._v(" 解决yarn-client模式导致的网卡流量激增的问题")]),t._v(" "),e("p",[t._v("在生产环境中用yarn-cluster模式去提交你的spark作业。")]),t._v(" "),e("h4",{attrs:{id:"正确的持久化使用方式"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#正确的持久化使用方式"}},[t._v("$")]),t._v(" 正确的持久化使用方式")]),t._v(" "),e("div",{staticClass:"language-scala extra-class"},[e("pre",{pre:!0,attrs:{class:"language-scala"}},[e("code",[t._v("usersRDD "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" usersRDD"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cache"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" cachedUsersRDD "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" usersRDD"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cache"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h2",{attrs:{id:"数据倾斜"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#数据倾斜"}},[t._v("$")]),t._v(" 数据倾斜")]),t._v(" "),e("h3",{attrs:{id:"现象"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#现象"}},[t._v("$")]),t._v(" 现象")]),t._v(" "),e("ul",[e("li",[t._v("绝大多数task执行得都非常快，但个别task执行特别慢")]),t._v(" "),e("li",[t._v("有的task会内存溢出")])]),t._v(" "),e("h3",{attrs:{id:"原因定位"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#原因定位"}},[t._v("$")]),t._v(" 原因定位")]),t._v(" "),e("p",[t._v("数据倾斜只会发生在shuffle过程中。常见会触发shuffle操作的算子有：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。")]),t._v(" "),e("p",[t._v("某个task执行特别慢的情况：")]),t._v(" "),e("p",[t._v("确定数据倾斜发生在第几个stage中，然后在Spark Web UI上深入看一下当前这个stage各个task分配的数据量")]),t._v(" "),e("p",[t._v("某个task内存溢出的情况：")]),t._v(" "),e("p",[t._v("查看log的异常栈来定位代码，然后通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量")]),t._v(" "),e("p",[t._v("查看导致数据倾斜的key的数据分布情况：")]),t._v(" "),e("p",[t._v("根据你执行操作的情况不同，可以有很多种查看key分布的方式：")]),t._v(" "),e("ol",[e("li",[t._v("如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。")]),t._v(" "),e("li",[t._v("如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入代码抽样统计。")])]),t._v(" "),e("h3",{attrs:{id:"解决方案"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#解决方案"}},[t._v("$")]),t._v(" 解决方案")]),t._v(" "),e("h4",{attrs:{id:"方案一-使用hive-etl预处理数据"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方案一-使用hive-etl预处理数据"}},[t._v("$")]),t._v(" 方案一：使用Hive ETL预处理数据")]),t._v(" "),e("h4",{attrs:{id:"方案二-过滤少数导致倾斜的key"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方案二-过滤少数导致倾斜的key"}},[t._v("$")]),t._v(" 方案二：过滤少数导致倾斜的key")]),t._v(" "),e("p",[t._v("如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。")]),t._v(" "),e("p",[t._v("如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。")]),t._v(" "),e("h4",{attrs:{id:"方案三-提高shuffle操作的并行度"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方案三-提高shuffle操作的并行度"}},[t._v("$")]),t._v(" 方案三：提高shuffle操作的并行度")]),t._v(" "),e("p",[t._v("增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。")]),t._v(" "),e("p",[t._v("在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。")]),t._v(" "),e("p",[t._v("对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置参数spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，默认是200。")]),t._v(" "),e("h4",{attrs:{id:"方案四-两阶段聚合"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方案四-两阶段聚合"}},[t._v("$")]),t._v(" 方案四：两阶段聚合")]),t._v(" "),e("p",[t._v("将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。"),e("strong",[t._v("适用于reduceByKey等聚合类shuffle算子")]),t._v("。")]),t._v(" "),e("h4",{attrs:{id:"方案五-将reduce-join转为map-join"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方案五-将reduce-join转为map-join"}},[t._v("$")]),t._v(" 方案五：将reduce join转为map join")]),t._v(" "),e("p",[t._v("如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。")]),t._v(" "),e("h4",{attrs:{id:"方案六-采样倾斜key并分拆join操作"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方案六-采样倾斜key并分拆join操作"}},[t._v("$")]),t._v(" 方案六：采样倾斜key并分拆join操作")]),t._v(" "),e("ul",[e("li",[t._v("对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。")]),t._v(" "),e("li",[t._v("然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。")]),t._v(" "),e("li",[t._v("接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。")]),t._v(" "),e("li",[t._v("再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。")]),t._v(" "),e("li",[t._v("而另外两个普通的RDD就照常join即可。")]),t._v(" "),e("li",[t._v("最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。")])]),t._v(" "),e("h4",{attrs:{id:"方案七-使用随机前缀和扩容rdd进行join"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#方案七-使用随机前缀和扩容rdd进行join"}},[t._v("$")]),t._v(" 方案七：使用随机前缀和扩容RDD进行join")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("该方案的实现思路基本和方案六类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。")])]),t._v(" "),e("li",[e("p",[t._v("然后将该RDD的每条数据都打上一个n以内的随机前缀。")])]),t._v(" "),e("li",[e("p",[t._v("同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。")])]),t._v(" "),e("li",[e("p",[t._v("最后将两个处理后的RDD进行join即可。")])])]),t._v(" "),e("h3",{attrs:{id:"参考-4"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考-4"}},[t._v("$")]),t._v(" 参考")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/71270044",target:"_blank",rel:"noopener noreferrer"}},[t._v("spark知识点总结"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://tech.meituan.com/2016/05/12/spark-tuning-pro.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark性能优化指南——高级篇"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"原理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#原理"}},[t._v("$")]),t._v(" 原理")]),t._v(" "),e("h3",{attrs:{id:"join原理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#join原理"}},[t._v("$")]),t._v(" Join原理")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("类型")]),t._v(" "),e("th",[t._v("特点")]),t._v(" "),e("th",[t._v("适用场景")]),t._v(" "),e("th",[t._v("相关配置参数")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Broadcast Hash Join")]),t._v(" "),e("td",[t._v("没有Shuffle")]),t._v(" "),e("td",[t._v("等值Join，不能用于Full Outer Join")]),t._v(" "),e("td",[t._v("spark.sql.autoBroadcastJoinThreshold=10M")])]),t._v(" "),e("tr",[e("td",[t._v("Sort Merge Join")]),t._v(" "),e("td",[t._v("有Shuffle，内存比Hash Join要少")]),t._v(" "),e("td",[t._v("只适用等值Join，且Join条件中的key是可排序的")]),t._v(" "),e("td",[t._v("spark.sql.join.prefersortmergeJoin=true")])]),t._v(" "),e("tr",[e("td",[t._v("Shuffle Hash Join")]),t._v(" "),e("td",[t._v("有Shuffle")]),t._v(" "),e("td",[t._v("等值Join，不能用于Full Outer Join")]),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("Broadcast Nested Join")]),t._v(" "),e("td",[t._v("需要广播数据集和嵌套循环")]),t._v(" "),e("td",[t._v("默认")]),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("Cartesian Join")]),t._v(" "),e("td",[t._v("结果的分区数等于输入数据集的分区数之积")]),t._v(" "),e("td",[t._v("Cross Join")]),t._v(" "),e("td")])])]),t._v(" "),e("h4",{attrs:{id:"参考-5"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考-5"}},[t._v("$")]),t._v(" 参考")]),t._v(" "),e("p",[e("a",{attrs:{href:"http://hbasefly.com/2017/03/19/sparksql-basic-join/",target:"_blank",rel:"noopener noreferrer"}},[t._v("SparkSQL – 有必要坐下来聊聊Join"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://jiamaoxiang.top/2020/11/01/Spark%E7%9A%84%E4%BA%94%E7%A7%8DJOIN%E6%96%B9%E5%BC%8F%E8%A7%A3%E6%9E%90/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark的五种JOIN策略解析"),e("OutboundLink")],1)]),t._v(" "),e("h3",{attrs:{id:"shuffle原理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#shuffle原理"}},[t._v("$")]),t._v(" Shuffle原理")]),t._v(" "),e("p",[t._v("在spark中，发生shuffle操作主要是以下几个算子：")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("算子")]),t._v(" "),e("th",[t._v("操作")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("groupByKey")]),t._v(" "),e("td",[t._v("要把分布在集群各个节点上的数据中的同一个key，对应的values，都集中到一个节点的一个executor的一个task中处理")])]),t._v(" "),e("tr",[e("td",[t._v("reduceByKey")]),t._v(" "),e("td",[t._v("算子函数去对values集合进行reduce操作，最后变成一个value")])]),t._v(" "),e("tr",[e("td",[t._v("countByKey")]),t._v(" "),e("td",[t._v("需要在一个task中，获取到一个key对应的所有的value，然后进行计数，统计总共有多少个value；")])]),t._v(" "),e("tr",[e("td",[t._v("join")]),t._v(" "),e("td",[t._v("只要是两个RDD中，key相同对应的2个value，都能到一个节点的executor的task中，给我们进行处理。")])])])]),t._v(" "),e("p",[t._v("reduceByKey("),e("em",[t._v("+")]),t._v(")，在某个action触发job的时候，DAGScheduler，会负责将job划分为多个stage。划分的依据，就是，如果发现有会触发shuffle操作的算子，比如reduceByKey，就将这个操作的前半部分，以及之前所有的RDD和transformation操作，划分为一个stage；shuffle操作的后半部分，以及后面的，直到action为止的RDD和transformation操作，划分为另外一个stage。")]),t._v(" "),e("p",[t._v("每一个shuffle的前半部分stage的task，每个task都会创建下一个stage的task数量相同的文件，比如下一个stage会有100个task，那么当前stage每个task都会创建100份文件；会将同一个key对应的values，一定是写入同一个文件中的；不同节点上的task，也一定会将同一个key对应的values，写入下一个stage，同一个task对应的文件中。")]),t._v(" "),e("p",[t._v("shuffle的后半部分stage的task，每个task都会从各个节点上的task写的属于自己的那一份文件中，拉取key, value对；然后task会有一个内存缓冲区，然后会用HashMap，进行key, values的汇聚：(key ,values)；")]),t._v(" "),e("p",[t._v("task会用我们自己定义的聚合函数，比如reduceByKey("),e("em",[t._v("+")]),t._v(")，把所有values进行一对一的累加；聚合出来最终的值。就完成了shuffle。")]),t._v(" "),e("p",[t._v("shuffle前半部分的task在写入数据到磁盘文件之前，都会先写入一个一个的内存缓冲，内存缓冲满溢之后，再spill溢写到磁盘文件中。")]),t._v(" "),e("p",[t._v("默认Shuffle过程：")]),t._v(" "),e("p",[t._v("第一个stage，每个task，都会给第二个stage的每个task创建一份map端的输出文件。")]),t._v(" "),e("p",[t._v("第二个stage，每个task，会到各个节点上面去，拉取第一个stage每个task输出的，属于自己的那一份文件。")]),t._v(" "),e("p",[t._v("开启了map端输出文件的合并机制之后：")]),t._v(" "),e("p",[t._v("第一个stage，同时就运行cpu core个task，比如cpu core是2个，并行运行2个task；每个task都创建下一个stage的task数量个文件；")]),t._v(" "),e("p",[t._v("第一个stage，并行运行的2个task执行完以后；就会执行另外两个task；另外2个task不会再重新创建输出文件；而是复用之前的task创建的map端输出文件，将数据写入上一批task的输出文件中。")]),t._v(" "),e("p",[t._v("第二个stage，task在拉取数据的时候，就不会去拉取上一个stage每一个task为自己创建的那份输出文件了；而是拉取少量的输出文件，每个输出文件中，可能包含了多个task给自己的map端输出。")]),t._v(" "),e("p",[t._v("对比：")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("项")]),t._v(" "),e("th",[t._v("默认")]),t._v(" "),e("th",[t._v("开启map端合并")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("executor")]),t._v(" "),e("td",[t._v("100")]),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("cpu core")]),t._v(" "),e("td",[t._v("2")]),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("task")]),t._v(" "),e("td",[t._v("1000")]),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("单节点输出map端文件数")]),t._v(" "),e("td",[t._v("1000/100 * 1000")]),t._v(" "),e("td",[t._v("2 * 1000")])]),t._v(" "),e("tr",[e("td",[t._v("总节点输出map端文件数")]),t._v(" "),e("td",[t._v("100 * 10000")]),t._v(" "),e("td",[t._v("100 * 2000")])])])]),t._v(" "),e("p",[t._v("合并map端输出文件，对咱们的spark的性能有哪些方面的影响呢？")]),t._v(" "),e("ul",[e("li",[t._v("减少map task写入磁盘文件的IO")]),t._v(" "),e("li",[t._v("减少网络传输的性能消耗")])]),t._v(" "),e("h4",{attrs:{id:"hadoop和spark-shuffle机制对比"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#hadoop和spark-shuffle机制对比"}},[t._v("$")]),t._v(" Hadoop和Spark Shuffle机制对比")]),t._v(" "),e("p",[t._v("参考："),e("a",{attrs:{href:"https://cloud.tencent.com/developer/news/447236",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hadoop和Spark Shuffle机制对比"),e("OutboundLink")],1)]),t._v(" "),e("h3",{attrs:{id:"内存管理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#内存管理"}},[t._v("$")]),t._v(" 内存管理")]),t._v(" "),e("p",[t._v("Spark 内存模型：")]),t._v(" "),e("p",[t._v("Spark在一个Executor中的内存分为三块，一块是execution内存，一块是storage内存，一块是other内存。")]),t._v(" "),e("ul",[e("li",[t._v("execution内存是执行内存，文档中说join，aggregate都在这部分内存中执行，shuffle的数据也会先缓存在这个内存中，满了再写入磁盘，能够减少IO。其实map过程也是在这个内存中执行的。")]),t._v(" "),e("li",[t._v("storage内存是存储broadcast，cache，persist数据的地方。")]),t._v(" "),e("li",[t._v("other内存是程序执行时预留给自己的内存。")])]),t._v(" "),e("p",[t._v("execution和storage是Spark Executor中内存的大户，other占用内存相对少很多，这里就不说了。在spark-1.6.0以前的版本，execution和storage的内存分配是固定的，使用的参数配置分别是spark.shuffle.memoryFraction（execution内存占Executor总内存大小，default 0.2）和spark.storage.memoryFraction（storage内存占Executor内存大小，default 0.6），因为是1.6.0以前这两块内存是互相隔离的，这就导致了Executor的内存利用率不高，而且需要根据Application的具体情况，使用者自己来调节这两个参数才能优化Spark的内存使用。在spark-1.6.0以上的版本，execution内存和storage内存可以相互借用，提高了内存的Spark中内存的使用率，同时也减少了OOM的情况。")]),t._v(" "),e("p",[t._v("在Spark-1.6.0后加入了堆外内存，进一步优化了Spark的内存使用，堆外内存使用JVM堆以外的内存，不会被gc回收，可以减少频繁的full gc，所以在Spark程序中，会长时间逗留再Spark程序中的大内存对象可以使用堆外内存存储。使用堆外内存有两种方式，一种是在rdd调用persist的时候传入参数StorageLevel.OFF_HEAP，这种使用方式需要配合Tachyon一起使用。另外一种是使用Spark自带的spark.memory.offHeap.enabled 配置为true进行使用，但是这种方式在1.6.0的版本还不支持使用，只是多了这个参数，在以后的版本中会开放。")]),t._v(" "),e("p",[t._v("OOM的问题通常出现在execution这块内存中，因为storage这块内存在存放数据满了之后，会直接丢弃内存中旧的数据，对性能有影响但是不会有OOM的问题。")]),t._v(" "),e("p",[t._v("参考："),e("a",{attrs:{href:"https://blog.csdn.net/yhb315279058/article/details/51035631",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark面对OOM问题的解决方法及优化总结"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("静态内存管理：")]),t._v(" "),e("img",{staticStyle:{zoom:"33%"},attrs:{src:a(326),alt:"img"}}),t._v(" "),e("img",{staticStyle:{zoom:"50%"},attrs:{src:a(327),alt:"img"}}),t._v(" "),e("p",[t._v("统一内存管理：")]),t._v(" "),e("img",{staticStyle:{zoom:"33%"},attrs:{src:a(328),alt:"img"}}),t._v(" "),e("img",{staticStyle:{zoom:"67%"},attrs:{src:a(329),alt:"img"}}),t._v(" "),e("p",[t._v("其中最重要的优化在于动态占用机制，其规则如下：")]),t._v(" "),e("ul",[e("li",[t._v("设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围")]),t._v(" "),e("li",[t._v("双方的空间都不足时，则存储到硬盘；若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）")]),t._v(" "),e("li",[t._v('执行内存的空间被对方占用后，可让对方将占用的部分转存到硬盘，然后"归还"借用的空间')]),t._v(" "),e("li",[t._v('存储内存的空间被对方占用后，无法让对方"归还"，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂')])]),t._v(" "),e("h4",{attrs:{id:"参考-6"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考-6"}},[t._v("$")]),t._v(" 参考")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.cnblogs.com/frankdeng/p/9301783.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark（七）Spark内存调优"),e("OutboundLink")],1)]),t._v(" "),e("h2",{attrs:{id:"问题排查"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#问题排查"}},[t._v("$")]),t._v(" 问题排查")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.jianshu.com/p/1f45bb8a81b3",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark 任务执行排查慢的问题排查-2"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.cnblogs.com/tian2fei/p/4285168.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark处理数据出现大量GC导致处理性能变慢的原因及解决方案"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://blog.csdn.net/qq_33160722/article/details/54092560",target:"_blank",rel:"noopener noreferrer"}},[t._v("最近在SPARK上定位的几个内存泄露问题总结"),e("OutboundLink")],1)])])}),[],!1,null,null,null);e.default=s.exports}}]);